{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8830ac5d",
   "metadata": {},
   "source": [
    "# Structured RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "760218a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74d65e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "class CalendarEvent(BaseModel):\n",
    "    name: str\n",
    "    date: str\n",
    "    participants: list[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e661c48c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 385 chunks from 95 documents\n"
     ]
    }
   ],
   "source": [
    "from gitsource import GithubRepositoryDataReader, chunk_documents\n",
    "from minsearch import Index\n",
    "\n",
    "reader = GithubRepositoryDataReader(\n",
    "    repo_owner=\"evidentlyai\",\n",
    "    repo_name=\"docs\",\n",
    "    allowed_extensions={\"md\", \"mdx\"},\n",
    ")\n",
    "files = reader.read()\n",
    "\n",
    "parsed_docs = [doc.parse() for doc in files]\n",
    "chunked_docs = chunk_documents(parsed_docs, size=3000, step=1500)\n",
    "\n",
    "index = Index(\n",
    "    text_fields=[\"title\", \"description\", \"content\"],\n",
    "    keyword_fields=[\"filename\"]\n",
    ")\n",
    "index.fit(chunked_docs)\n",
    "\n",
    "print(f\"Indexed {len(chunked_docs)} chunks from {len(files)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1206ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        num_results=5\n",
    "    )\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4772390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "instructions = \"\"\"\n",
    "You're a documentation assistant. Answer the QUESTION based on the CONTEXT from our documentation.\n",
    "\n",
    "Use only facts from the CONTEXT when answering.\n",
    "If the answer isn't in the CONTEXT, say so.\n",
    "\"\"\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(question, search_results):\n",
    "    context = json.dumps(search_results, indent=2)\n",
    "    return prompt_template.format(\n",
    "        question=question,\n",
    "        context=context\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c16788a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(user_prompt, instructions=None, model=\"gpt-4o-mini\"):\n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e00c89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    return llm(prompt, instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8773d223",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag('how do i implement llm as a judge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aad2e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_structured(\n",
    "        user_prompt,\n",
    "        output_type =None,\n",
    "        instructions=None,\n",
    "        model='gpt-4o-mini',\n",
    "        ):\n",
    "    \n",
    "    messages = []\n",
    "\n",
    "    if instructions:\n",
    "        messages.append({\n",
    "            \"role\": \"system\",\n",
    "            \"content\": instructions\n",
    "        })\n",
    "\n",
    "    messages.append({\n",
    "        \"role\": \"user\",\n",
    "        \"content\": user_prompt\n",
    "    })\n",
    "\n",
    "    response = openai_client.responses.parse(\n",
    "        model=model,\n",
    "        input=messages,\n",
    "        text_format=output_type\n",
    "    )\n",
    "\n",
    "    return response.output_parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8a35387d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = llm_structured(\n",
    "    instructions='Extract the event information.',\n",
    "    user_prompt='Alice and Bob are going to a science fair on Friday',\n",
    "    output_type=CalendarEvent\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1040b1e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CalendarEvent(name='Science Fair', date='Friday', participants=['Alice', 'Bob'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7508114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagResponse(BaseModel):\n",
    "    answer: str\n",
    "    found_answer: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "336682e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_structured(query, output_type = RagResponse):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "\n",
    "    return llm_structured(instructions = instructions,\n",
    "                           user_prompt= prompt, \n",
    "                           output_type = output_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a1ee6b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To perform LLM evaluations, you can follow these steps:\n",
      "\n",
      "1. **Installation and Imports**: Install th\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "answer = rag_structured('how do i do llm evals?')\n",
    "\n",
    "print(answer.answer[:100])\n",
    "print(answer.found_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7b016ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The CONTEXT does not provide any information on how to install Kafka on Windows.\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "answer = rag_structured('how do i isntall kafka on windows?')\n",
    "\n",
    "print(answer.answer[:100])\n",
    "print(answer.found_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "90f4ec3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'answer': {'title': 'Answer', 'type': 'string'},\n",
       "  'found_answer': {'title': 'Found Answer', 'type': 'boolean'}},\n",
       " 'required': ['answer', 'found_answer'],\n",
       " 'title': 'RagResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RagResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4071469",
   "metadata": {},
   "source": [
    "## Add optional on class RagResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fa7b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class RagResponse(BaseModel):\n",
    "    answer:Optional[str] = None\n",
    "    found_answer: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b34972c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'properties': {'answer': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Answer'},\n",
       "  'found_answer': {'title': 'Found Answer', 'type': 'boolean'}},\n",
       " 'required': ['found_answer'],\n",
       " 'title': 'RagResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RagResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c7f46a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "answer = rag_structured('how do i isntall kafka on windows?', RagResponse)\n",
    "\n",
    "print(answer.answer)\n",
    "print(answer.found_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f861663",
   "metadata": {},
   "source": [
    "## Add into instructions answer None, if llm dont know the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80ae2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're a documentation assistant. Answer the QUESTION based on the CONTEXT from our documentation.\n",
    "\n",
    "Use only facts from the CONTEXT when answering.\n",
    "If the answer isn't in the CONTEXT, say so.\n",
    "\n",
    "If you don't know the answer, set 'answer' to None\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1755e6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "answer = rag_structured('how do i isntall kafka on windows?', RagResponse)\n",
    "\n",
    "print(answer.answer)\n",
    "print(answer.found_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97addf4f",
   "metadata": {},
   "source": [
    "## Add into class RagResponse If the answer to the question wasn't found in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57e3d73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're a documentation assistant. Answer the QUESTION based on the CONTEXT from our documentation.\n",
    "\n",
    "Use only facts from the CONTEXT when answering.\n",
    "If the answer isn't in the CONTEXT, say so.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b831f78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "class RagResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    The response from the documentation RAG system\n",
    "    \"\"\"\n",
    "    answer:Optional[str] = None # If the answer to the question wasn't found in the database, 'answer' is None\n",
    "    found_answer: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d781e4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'The response from the documentation RAG system',\n",
       " 'properties': {'answer': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'title': 'Answer'},\n",
       "  'found_answer': {'title': 'Found Answer', 'type': 'boolean'}},\n",
       " 'required': ['found_answer'],\n",
       " 'title': 'RagResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RagResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3079da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import Field\n",
    "\n",
    "class RagResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    The response from the documentation RAG system\n",
    "    \"\"\"\n",
    "    answer:Optional[str] = Field(None, description=\"Answer to the question or None if it's not found\")\n",
    "    found_answer: bool = Field(description=\"True if the answer is found, False otherwise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b8ebb7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'The response from the documentation RAG system',\n",
       " 'properties': {'answer': {'anyOf': [{'type': 'string'}, {'type': 'null'}],\n",
       "   'default': None,\n",
       "   'description': \"Answer to the question or None if it's not found\",\n",
       "   'title': 'Answer'},\n",
       "  'found_answer': {'description': 'True if the answer is found, False otherwise',\n",
       "   'title': 'Found Answer',\n",
       "   'type': 'boolean'}},\n",
       " 'required': ['found_answer'],\n",
       " 'title': 'RagResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RagResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ea7205d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "answer = rag_structured('how do i isntall kafka on windows?', RagResponse)\n",
    "\n",
    "print(answer.answer)\n",
    "print(answer.found_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee07756b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "class RagResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    This model provides a structured answer with metadata about the response,\n",
    "    including confidence, categorization, and follow-up suggestions.\n",
    "    \"\"\"\n",
    "    answer: str = Field(description=\"The main answer to the user's question in markdown\")\n",
    "    found_answer: bool = Field(description=\"True if relevant information was found in the documentation\")\n",
    "    confidence: float = Field(description=\"Confidence score from 0.0 to 1.0 indicating how certain the answer is\")\n",
    "    confidence_explanation: str = Field(description=\"A brief explanation of the confidence score, highlighting key factors that influenced it\")\n",
    "    answer_type: Literal[\"how-to\",\"explanation\", \"troubleshoting\",\"comparison\",\"reference\"] = Field(description=\"The category of the answer\")\n",
    "    followup_questions: list[str] = Field(description=\"Suggested follow-up questions the user might want to ask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c560a85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'This model provides a structured answer with metadata about the response,\\nincluding confidence, categorization, and follow-up suggestions.',\n",
       " 'properties': {'answer': {'description': \"The main answer to the user's question in markdown\",\n",
       "   'title': 'Answer',\n",
       "   'type': 'string'},\n",
       "  'found_answer': {'description': 'True if relevant information was found in the documentation',\n",
       "   'title': 'Found Answer',\n",
       "   'type': 'boolean'},\n",
       "  'confidence': {'description': 'Confidence score from 0.0 to 1.0 indicating how certain the answer is',\n",
       "   'title': 'Confidence',\n",
       "   'type': 'number'},\n",
       "  'confidence_explanation': {'description': 'A brief explanation of the confidence score, highlighting key factors that influenced it',\n",
       "   'title': 'Confidence Explanation',\n",
       "   'type': 'string'},\n",
       "  'answer_type': {'description': 'The category of the answer',\n",
       "   'enum': ['how-to',\n",
       "    'explanation',\n",
       "    'troubleshoting',\n",
       "    'comparison',\n",
       "    'reference'],\n",
       "   'title': 'Answer Type',\n",
       "   'type': 'string'},\n",
       "  'followup_questions': {'description': 'Suggested follow-up questions the user might want to ask',\n",
       "   'items': {'type': 'string'},\n",
       "   'title': 'Followup Questions',\n",
       "   'type': 'array'}},\n",
       " 'required': ['answer',\n",
       "  'found_answer',\n",
       "  'confidence',\n",
       "  'confidence_explanation',\n",
       "  'answer_type',\n",
       "  'followup_questions'],\n",
       " 'title': 'RagResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RagResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "efd2bec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_structured('how do I evaluate llms', RagResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bba0ca27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To evaluate language models (LLMs), you can follow this structured approach:\n",
      "\n",
      "### 1. Set Up Evaluators with Multiple LLMs\n",
      "Use multiple LLMs to assess the same outputs. An output is considered a \"pass\" if all or the majority of LLMs approve, enabling you to see aggregate results and disagreements. Here’s how to set it up:\n",
      "   \n",
      "```python\n",
      "import os\n",
      "os.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\n",
      "os.environ[\"GEMINI_API_KEY\"] = \"YOUR KEY\"\n",
      "os.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\n",
      "```\n",
      "\n",
      "### 2. Dataset Preparation\n",
      "Define a dataset of user intents alongside generated emails to evaluate their appropriateness:\n",
      "\n",
      "```python\n",
      "data = [\n",
      "    [\"user input\", \"generated email\"],\n",
      "    ... \n",
      "]\n",
      "eval_df = pd.DataFrame(data, columns=[\"user input\", \"generated email\"])\n",
      "```\n",
      "\n",
      "### 3. Define the Evaluation Criteria\n",
      "Utilize an evaluation template to set the judging criteria.\n",
      "\n",
      "```python\n",
      "from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "\n",
      "criteria = BinaryClassificationPromptTemplate(\n",
      "    pre_messages=[(\"system\", \"You will be shown a snippet of an email...\")],\n",
      "    criteria=\"An APPROPRIATE email text is one that would be acceptable...\"\n",
      ")\n",
      "```\n",
      "\n",
      "### 4. Aggregate LLM Evaluation\n",
      "Create evaluators to score the email using different judges:\n",
      "\n",
      "```python\n",
      "llm_evals = Dataset.from_pandas(\n",
      "    eval_df,\n",
      "    data_definition=DataDefinition(),\n",
      "    descriptors=[\n",
      "        LLMEval(...),  # Definitions for each LLM\n",
      "        TestSummary(success_all=True, success_count=True, success_rate=True)\n",
      "    ]\n",
      ")\n",
      "```\n",
      "\n",
      "### 5. Run the Evaluation and Generate Report\n",
      "You can run the evaluation and obtain a summary report:\n",
      "\n",
      "```python\n",
      "report = Report([\n",
      "    TextEvals()\n",
      "])\n",
      "my_eval = report.run(llm_evals, None)\n",
      "```\n",
      "\n",
      "This process enables you to evaluate LLM outputs using various judges and under defined criteria while maintaining a record of their performance and any disagreements.\n"
     ]
    }
   ],
   "source": [
    "print(answer.answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54188975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RagResponse(answer='To evaluate language models (LLMs), you can follow this structured approach:\\n\\n### 1. Set Up Evaluators with Multiple LLMs\\nUse multiple LLMs to assess the same outputs. An output is considered a \"pass\" if all or the majority of LLMs approve, enabling you to see aggregate results and disagreements. Here’s how to set it up:\\n   \\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR KEY\"\\nos.environ[\"GEMINI_API_KEY\"] = \"YOUR KEY\"\\nos.environ[\"ANTHROPIC_API_KEY\"] = \"YOUR KEY\"\\n```\\n\\n### 2. Dataset Preparation\\nDefine a dataset of user intents alongside generated emails to evaluate their appropriateness:\\n\\n```python\\ndata = [\\n    [\"user input\", \"generated email\"],\\n    ... \\n]\\neval_df = pd.DataFrame(data, columns=[\"user input\", \"generated email\"])\\n```\\n\\n### 3. Define the Evaluation Criteria\\nUtilize an evaluation template to set the judging criteria.\\n\\n```python\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n\\ncriteria = BinaryClassificationPromptTemplate(\\n    pre_messages=[(\"system\", \"You will be shown a snippet of an email...\")],\\n    criteria=\"An APPROPRIATE email text is one that would be acceptable...\"\\n)\\n```\\n\\n### 4. Aggregate LLM Evaluation\\nCreate evaluators to score the email using different judges:\\n\\n```python\\nllm_evals = Dataset.from_pandas(\\n    eval_df,\\n    data_definition=DataDefinition(),\\n    descriptors=[\\n        LLMEval(...),  # Definitions for each LLM\\n        TestSummary(success_all=True, success_count=True, success_rate=True)\\n    ]\\n)\\n```\\n\\n### 5. Run the Evaluation and Generate Report\\nYou can run the evaluation and obtain a summary report:\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\nmy_eval = report.run(llm_evals, None)\\n```\\n\\nThis process enables you to evaluate LLM outputs using various judges and under defined criteria while maintaining a record of their performance and any disagreements.', found_answer=True, confidence=0.95, confidence_explanation='The answer is based directly on the detailed steps outlined in the provided context for evaluating LLMs using multiple judges and the accompanying code examples.', answer_type='how-to', followup_questions=['What metrics should I look for in the reports?', 'How do I integrate additional LLM providers?', 'Can I customize the evaluation criteria further?'])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6e48b421",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = rag_structured('how do I install kafka on windows?', RagResponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db82b8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The provided context does not include information on how to install Kafka on Windows. Please consult\n",
      "0.0\n",
      "The relevant documentation does not mention Kafka or its installation process, indicating a lack of information on this topic.\n",
      "reference\n",
      "['What is Kafka?', 'Can you help with Kafka configuration?', 'Where can I find Kafka installation documentation?']\n"
     ]
    }
   ],
   "source": [
    "print(answer.answer[:100])\n",
    "print(answer.confidence)\n",
    "print(answer.confidence_explanation)\n",
    "print(answer.answer_type)\n",
    "print(answer.followup_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c194a623",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import model_validator\n",
    "\n",
    "class AnswerNotFound(BaseModel):\n",
    "    explanation: str\n",
    "\n",
    "class AnswerResponse(BaseModel):\n",
    "    \"\"\"\n",
    "    If answer is found, 'answer' is populated.\n",
    "    If no answer is found, 'answer_not_found' is populated.\n",
    "    Only one of the two fields can be set at a time. Never both or neither.\n",
    "    \"\"\"\n",
    "\n",
    "    answer_not_found: Optional[AnswerNotFound] = None\n",
    "    found_answer: bool\n",
    "    answer: Optional[RagResponse] = None\n",
    "\n",
    "    @model_validator(mode=\"after\")\n",
    "    def check_consistency(self):\n",
    "        if self.answer is not None and self.answer_not_found is not None:\n",
    "            raise ValueError(\"Provide either 'answer' or 'answer_not_found', not both.\")\n",
    "\n",
    "        if self.answer is None and self.answer_not_found is None:\n",
    "            raise ValueError(\"Provide either 'answer' or 'answer_not_found'.\")\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e31b3907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'description': 'This model provides a structured answer with metadata about the response,\\nincluding confidence, categorization, and follow-up suggestions.',\n",
       " 'properties': {'answer': {'description': \"The main answer to the user's question in markdown\",\n",
       "   'title': 'Answer',\n",
       "   'type': 'string'},\n",
       "  'found_answer': {'description': 'True if relevant information was found in the documentation',\n",
       "   'title': 'Found Answer',\n",
       "   'type': 'boolean'},\n",
       "  'confidence': {'description': 'Confidence score from 0.0 to 1.0 indicating how certain the answer is',\n",
       "   'title': 'Confidence',\n",
       "   'type': 'number'},\n",
       "  'confidence_explanation': {'description': 'A brief explanation of the confidence score, highlighting key factors that influenced it',\n",
       "   'title': 'Confidence Explanation',\n",
       "   'type': 'string'},\n",
       "  'answer_type': {'description': 'The category of the answer',\n",
       "   'enum': ['how-to',\n",
       "    'explanation',\n",
       "    'troubleshoting',\n",
       "    'comparison',\n",
       "    'reference'],\n",
       "   'title': 'Answer Type',\n",
       "   'type': 'string'},\n",
       "  'followup_questions': {'description': 'Suggested follow-up questions the user might want to ask',\n",
       "   'items': {'type': 'string'},\n",
       "   'title': 'Followup Questions',\n",
       "   'type': 'array'}},\n",
       " 'required': ['answer',\n",
       "  'found_answer',\n",
       "  'confidence',\n",
       "  'confidence_explanation',\n",
       "  'answer_type',\n",
       "  'followup_questions'],\n",
       " 'title': 'RagResponse',\n",
       " 'type': 'object'}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RagResponse.model_json_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2664a203",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerResponse(answer_not_found=AnswerNotFound(explanation='The provided documentation does not contain information regarding the installation of Kafka on Windows.'), found_answer=False, answer=None)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = rag_structured('how do I install kafka on windows?', AnswerResponse)\n",
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "89638e17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AnswerResponse(answer_not_found=None, found_answer=True, answer=RagResponse(answer='To run LLM evaluations, follow these steps:\\n\\n1. **Connect to Evidently Cloud and Create a Project**:\\n   You must first connect to [Evidently Cloud](/docs/setup/cloud) and [create a Project](/docs/platform/projects_manage).\\n\\n2. **Prepare the Dataset**:\\n   Ensure you have a dataset with input questions and computed descriptors. You can create a dataset in a pandas DataFrame, for example:\\n   ```python\\n   data = [\\n       [\"Question 1?\", \"Expected Response 1\"],\\n       [\"Question 2?\", \"Expected Response 2\"]\\n   ]\\n   columns = [\"question\", \"target_response\"]\\n   ref_data = pd.DataFrame(data, columns=columns)\\n   ```\\n\\n3. **Create and Run the Report**:\\n   Use the `TextEvals` Preset and run a report with your dataset. Here’s how you can do it:\\n   ```python\\n   from evidently.report import Report\\n   report = Report([\\n       TextEvals(),\\n   ])\\n   my_eval = report.run(ref_dataset, None)\\n   ```\\n   This will render the report directly in your Python environment.\\n\\n4. **Explore the Results**:\\n   After running your report, go to the Explore view inside your Project to debug the results and compare outcomes between runs. You can also set up dashboards and alerts as needed.', found_answer=True, confidence=0.85, confidence_explanation='The answer is based on the documented workflow for running evaluations, ensuring all required steps are included.', answer_type='how-to', followup_questions=['Can you provide more details about creating descriptors?', 'What metrics can I include in my evaluation?', 'How do I set up alerts for my evaluations?']))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer = rag_structured('how do I run llm evals?', AnswerResponse)\n",
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982e8a00",
   "metadata": {},
   "source": [
    "## Valdiation error for logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "247b8d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation error as expected: 1 validation error for AnswerResponse\n",
      "found_answer\n",
      "  Field required [type=missing, input_value={}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/missing\n"
     ]
    }
   ],
   "source": [
    "from pydantic import ValidationError\n",
    "\n",
    "try:\n",
    "    AnswerResponse()\n",
    "except ValidationError as e:\n",
    "    print(\"Validation error as expected:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b689a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-buildcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

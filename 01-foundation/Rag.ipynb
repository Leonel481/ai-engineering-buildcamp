{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d100cf48",
   "metadata": {},
   "source": [
    "# RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9a30e6",
   "metadata": {},
   "source": [
    "RAG consists of three steps:\n",
    "\n",
    "1. RETRIEVAL - Find relevant documents using search\n",
    "2. AUGMENTATION - Include the documents in the prompt\n",
    "3. GENERATION - LLM generates an answer using the retrieved context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ffa219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f724f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_owner = 'evidentlyai'\n",
    "repo_name = 'docs'\n",
    "branch_name = 'main'\n",
    "\n",
    "zip_url = f'https://github.com/{repo_owner}/{repo_name}/archive/refs/heads/{branch_name}.zip'\n",
    "zip_response = requests.get(zip_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5c3da91c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17545668"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zip_response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "404c6392",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import zipfile\n",
    "\n",
    "zip_archive = zipfile.ZipFile(io.BytesIO(zip_response.content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "478bc1cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs-main/docs/library/report.mdx',\n",
       " 'docs-main/docs/library/synthetic_data_api.mdx',\n",
       " 'docs-main/docs/library/tags_metadata.mdx',\n",
       " 'docs-main/docs/library/tests.mdx',\n",
       " 'docs-main/docs/platform/',\n",
       " 'docs-main/docs/platform/alerts.mdx',\n",
       " 'docs-main/docs/platform/dashboard_add_panels.mdx',\n",
       " 'docs-main/docs/platform/dashboard_add_panels_ui.mdx',\n",
       " 'docs-main/docs/platform/dashboard_overview.mdx',\n",
       " 'docs-main/docs/platform/dashboard_panel_types.mdx']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = zip_archive.namelist()\n",
    "filenames[20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "839e360b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'docs-main/docs/platform/alerts.mdx'\n",
    "\n",
    "mdx_file = zip_archive.open(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ca48438e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "title: 'Alerts'\n",
      "description: 'How to set up alerts.'\n",
      "---\n",
      "\n",
      "<Check>\n",
      "  Built-in alerting is a Pro feature available in the **Evidently Cloud** and **Evidently Enterprise**.\n",
      "</Check>\n",
      "\n",
      "![](/images/alerts.png)\n",
      "\n",
      "To enable alerts, open the Project and navigate to the \"Alerts\" in the left menu. You must set:\n",
      "\n",
      "* A notification channel.\n",
      "\n",
      "* An alert condition.\n",
      "\n",
      "## Notification channels\n",
      "\n",
      "You can choose between the following options:\n",
      "\n",
      "* **Email**. Add email addresses to send alerts to.\n",
      "\n",
      "* **Slack**. Add a Slack webhook.\n",
      "\n",
      "* **Discord**. Add a Discord webhook.\n",
      "\n",
      "## Alert conditions\n",
      "\n",
      "### Failed tests\n",
      "\n",
      "If you use Tests (conditional checks) in your Project, you can tie alerting to the failed Tests in a Test Suite. Toggle this option on the Alerts page. Evidently will set an alert to the defined channel if any of the Tests fail.\n",
      "\n",
      "<Tip>\n",
      "  **How to avoid alert fatigue?** Use the `is_critical` parameter to mark non-critical Test as Warnings. Setting it to `False` prevent alerts for those checks even if they fail.\n",
      "</Tip>\n",
      "\n",
      "### Custom conditions\n",
      "\n",
      "You can also set alerts on individual Metric values. For example, you can generate Alerts when the share of drifting features is above a certain threshold.\n",
      "\n",
      "Click on the plus sign below the “Add new Metric alert” and follow the prompts to set an alert condition.\n",
      "\n",
      "![](../.gitbook/assets/cloud/alerts.png)\n"
     ]
    }
   ],
   "source": [
    "mdx_content = mdx_file.read().decode('utf8')\n",
    "print(mdx_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5d3065a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Check>\n",
      "  Built-in alerting is a Pro feature available in the **Evidently Cloud** and **Evidently En\n"
     ]
    }
   ],
   "source": [
    "import frontmatter\n",
    "\n",
    "post = frontmatter.loads(mdx_content)\n",
    "print(post.content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f5f50a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Alerts', 'description': 'How to set up alerts.'}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ee64e4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs/platform/alerts.mdx\n"
     ]
    }
   ],
   "source": [
    "_, filename_corrected = filename.split('/', maxsplit=1)\n",
    "print(filename_corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41468366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['docs-main/',\n",
       " 'docs-main/api-reference/',\n",
       " 'docs-main/api-reference/endpoint/',\n",
       " 'docs-main/api-reference/endpoint/create.mdx',\n",
       " 'docs-main/api-reference/endpoint/delete.mdx',\n",
       " 'docs-main/api-reference/endpoint/get.mdx',\n",
       " 'docs-main/api-reference/introduction.mdx',\n",
       " 'docs-main/api-reference/openapi.json',\n",
       " 'docs-main/changelog/',\n",
       " 'docs-main/changelog/changelog.mdx',\n",
       " 'docs-main/docs/',\n",
       " 'docs-main/docs/library/',\n",
       " 'docs-main/docs/library/data_definition.mdx',\n",
       " 'docs-main/docs/library/descriptors.mdx',\n",
       " 'docs-main/docs/library/evaluations_overview.mdx',\n",
       " 'docs-main/docs/library/leftover_content.mdx',\n",
       " 'docs-main/docs/library/metric_generator.mdx',\n",
       " 'docs-main/docs/library/output_formats.mdx',\n",
       " 'docs-main/docs/library/overview.mdx',\n",
       " 'docs-main/docs/library/prompt_optimization.mdx',\n",
       " 'docs-main/docs/library/report.mdx',\n",
       " 'docs-main/docs/library/synthetic_data_api.mdx',\n",
       " 'docs-main/docs/library/tags_metadata.mdx',\n",
       " 'docs-main/docs/library/tests.mdx',\n",
       " 'docs-main/docs/platform/',\n",
       " 'docs-main/docs/platform/alerts.mdx',\n",
       " 'docs-main/docs/platform/dashboard_add_panels.mdx',\n",
       " 'docs-main/docs/platform/dashboard_add_panels_ui.mdx',\n",
       " 'docs-main/docs/platform/dashboard_overview.mdx',\n",
       " 'docs-main/docs/platform/dashboard_panel_types.mdx',\n",
       " 'docs-main/docs/platform/datasets_generate.mdx',\n",
       " 'docs-main/docs/platform/datasets_overview.mdx',\n",
       " 'docs-main/docs/platform/datasets_workflow.mdx',\n",
       " 'docs-main/docs/platform/evals_api.mdx',\n",
       " 'docs-main/docs/platform/evals_explore.mdx',\n",
       " 'docs-main/docs/platform/evals_no_code.mdx',\n",
       " 'docs-main/docs/platform/evals_overview.mdx',\n",
       " 'docs-main/docs/platform/monitoring_local_batch.mdx',\n",
       " 'docs-main/docs/platform/monitoring_overview.mdx',\n",
       " 'docs-main/docs/platform/monitoring_scheduled_evals.mdx',\n",
       " 'docs-main/docs/platform/overview.mdx',\n",
       " 'docs-main/docs/platform/projects_manage.mdx',\n",
       " 'docs-main/docs/platform/projects_overview.mdx',\n",
       " 'docs-main/docs/platform/tracing_overview.mdx',\n",
       " 'docs-main/docs/platform/tracing_setup.mdx',\n",
       " 'docs-main/docs/setup/',\n",
       " 'docs-main/docs/setup/cloud.mdx',\n",
       " 'docs-main/docs/setup/installation.mdx',\n",
       " 'docs-main/docs/setup/self-hosting.mdx',\n",
       " 'docs-main/examples/',\n",
       " 'docs-main/examples/GitHub_actions.mdx',\n",
       " 'docs-main/examples/LLM_evals.mdx',\n",
       " 'docs-main/examples/LLM_judge.mdx',\n",
       " 'docs-main/examples/LLM_jury.mdx',\n",
       " 'docs-main/examples/LLM_rag_evals.mdx',\n",
       " 'docs-main/examples/LLM_regression_testing.mdx',\n",
       " 'docs-main/examples/introduction.mdx',\n",
       " 'docs-main/faq/',\n",
       " 'docs-main/faq/cloud_v2.mdx',\n",
       " 'docs-main/faq/contact.mdx',\n",
       " 'docs-main/faq/introduction.mdx',\n",
       " 'docs-main/faq/migration.mdx',\n",
       " 'docs-main/faq/oss_vs_cloud.mdx',\n",
       " 'docs-main/faq/telemetry.mdx',\n",
       " 'docs-main/faq/why_evidently.mdx',\n",
       " 'docs-main/images/',\n",
       " 'docs-main/images/alerts.png',\n",
       " 'docs-main/images/changelog/',\n",
       " 'docs-main/images/changelog/editable_dataset-min.png',\n",
       " 'docs-main/images/changelog/readme.md',\n",
       " 'docs-main/images/concepts/',\n",
       " 'docs-main/images/concepts/evidently_oss_ui-min.png',\n",
       " 'docs-main/images/concepts/overview_descriptor_test_example-min.png',\n",
       " 'docs-main/images/concepts/overview_descriptors_export.png',\n",
       " 'docs-main/images/concepts/overview_drift_report-min.png',\n",
       " 'docs-main/images/concepts/overview_reports-min.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_cat_value_compare_example.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_cat_value_example.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_correlation_example-min.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_descriptor_example.png',\n",
       " 'docs-main/images/concepts/overview_small_preset_num_value_example-min.png',\n",
       " 'docs-main/images/concepts/overview_test_example-min.png',\n",
       " 'docs-main/images/concepts/overview_test_suite_example-min.png',\n",
       " 'docs-main/images/concepts/readme.md',\n",
       " 'docs-main/images/concepts/report_test_preview.gif',\n",
       " 'docs-main/images/concepts/text_data_drift_domain_classifier.png',\n",
       " 'docs-main/images/dashboard/',\n",
       " 'docs-main/images/dashboard/add_dashboard_tab.gif',\n",
       " 'docs-main/images/dashboard/add_dashboard_tab_v2.gif',\n",
       " 'docs-main/images/dashboard/add_panel_ui.png',\n",
       " 'docs-main/images/dashboard/add_panel_ui_pie.png',\n",
       " 'docs-main/images/dashboard/dashboard_to_report.gif',\n",
       " 'docs-main/images/dashboard/distribution_panels.png',\n",
       " 'docs-main/images/dashboard/metric_panels.png',\n",
       " 'docs-main/images/dashboard/panel_bar_plot_example.png',\n",
       " 'docs-main/images/dashboard/panel_counter_example-min.png',\n",
       " 'docs-main/images/dashboard/panel_dist_group_2-min.png',\n",
       " 'docs-main/images/dashboard/panel_dist_overlay-min.png',\n",
       " 'docs-main/images/dashboard/panel_dist_relative-min.png',\n",
       " 'docs-main/images/dashboard/panel_dist_stacked_2-min.png',\n",
       " 'docs-main/images/dashboard/panel_hist_example.png',\n",
       " 'docs-main/images/dashboard/panel_line_chart.png',\n",
       " 'docs-main/images/dashboard/panel_line_plot_example.png',\n",
       " 'docs-main/images/dashboard/panel_pie_chart.png',\n",
       " 'docs-main/images/dashboard/panel_scatter_plot_example.png',\n",
       " 'docs-main/images/dashboard/panel_tests_aggregated_hover_example.png',\n",
       " 'docs-main/images/dashboard/panel_tests_counter_example.png',\n",
       " 'docs-main/images/dashboard/panel_tests_detailed_hover_example.png',\n",
       " 'docs-main/images/dashboard/readme.md',\n",
       " 'docs-main/images/dashboard/test_panels.png',\n",
       " 'docs-main/images/dashboard_llm_dark.png',\n",
       " 'docs-main/images/dashboard_llm_light.png',\n",
       " 'docs-main/images/dashboard_llm_tabs.gif',\n",
       " 'docs-main/images/dataset_llm.png',\n",
       " 'docs-main/images/datasets_input_data_two.png',\n",
       " 'docs-main/images/evals_browse_reports-min.png',\n",
       " 'docs-main/images/evals_explore_view-min.png',\n",
       " 'docs-main/images/evals_flow_nocode.png',\n",
       " 'docs-main/images/evals_flow_python.png',\n",
       " 'docs-main/images/evals_no_code_add_descriptors-min.png',\n",
       " 'docs-main/images/examples/',\n",
       " 'docs-main/images/examples/dashboard_quickstart.png',\n",
       " 'docs-main/images/examples/data_drift_quickstart.png',\n",
       " 'docs-main/images/examples/github_actions.gif',\n",
       " 'docs-main/images/examples/hf_descriptor_example_toxicity-min.png',\n",
       " 'docs-main/images/examples/llm_judge_example_appropriate_question-min.png',\n",
       " 'docs-main/images/examples/llm_judge_example_context_quality-min.png',\n",
       " 'docs-main/images/examples/llm_judge_example_hallucination-min.png',\n",
       " 'docs-main/images/examples/llm_judge_example_multi_class_relevance.png',\n",
       " 'docs-main/images/examples/llm_judge_example_multi_class_safety.png',\n",
       " 'docs-main/images/examples/llm_judge_example_toxicity-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_cloud-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_conf_matrix-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_data_preview-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_judge_label_dist-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_judge_scored_data-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_report-min.png',\n",
       " 'docs-main/images/examples/llm_judge_tutorial_verbosity-min.png',\n",
       " 'docs-main/images/examples/llm_jury_example.png',\n",
       " 'docs-main/images/examples/llm_jury_overview.png',\n",
       " 'docs-main/images/examples/llm_quickstart_create_tab.gif',\n",
       " 'docs-main/images/examples/llm_quickstart_create_tab_new.gif',\n",
       " 'docs-main/images/examples/llm_quickstart_descriptor_custom_llm_judge-min.png',\n",
       " 'docs-main/images/examples/llm_quickstart_descriptor_tests-min.png',\n",
       " 'docs-main/images/examples/llm_quickstart_descriptor_tests_report-min.png',\n",
       " 'docs-main/images/examples/llm_quickstart_edit.png',\n",
       " 'docs-main/images/examples/llm_quickstart_explore.png',\n",
       " 'docs-main/images/examples/llm_quickstart_preview.png',\n",
       " 'docs-main/images/examples/llm_quickstart_report.png',\n",
       " 'docs-main/images/examples/llm_quickstart_tests.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_dashboard-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_data_preview-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_data_stats-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_new_data-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_report1-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_scored-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_stats_report-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_style-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_tests1-min.png',\n",
       " 'docs-main/images/examples/llm_regression_tutorial_tests2-min.png',\n",
       " 'docs-main/images/examples/rag_cloud_view-min.png',\n",
       " 'docs-main/images/examples/rag_correctness-min.png',\n",
       " 'docs-main/images/examples/rag_faithfulness-min.png',\n",
       " 'docs-main/images/examples/rag_multi_context_hit-min.png',\n",
       " 'docs-main/images/examples/rag_multi_context_mean-min.png',\n",
       " 'docs-main/images/examples/rag_reports-min.png',\n",
       " 'docs-main/images/examples/rag_single_context_hit-min.png',\n",
       " 'docs-main/images/examples/rag_single_context_valid-min.png',\n",
       " 'docs-main/images/examples/rag_tests-min.png',\n",
       " 'docs-main/images/examples/readme.md',\n",
       " 'docs-main/images/examples/tracing_tutorial_dataset_view.png',\n",
       " 'docs-main/images/examples/tracing_tutorial_evals.png',\n",
       " 'docs-main/images/examples/tracing_tutorial_session_view.png',\n",
       " 'docs-main/images/examples/tracing_tutorial_traces_view.png',\n",
       " 'docs-main/images/library_small-min.png',\n",
       " 'docs-main/images/metrics/',\n",
       " 'docs-main/images/metrics/descriptors-min.png',\n",
       " 'docs-main/images/metrics/descriptors-report-test.png',\n",
       " 'docs-main/images/metrics/descriptors-report.png',\n",
       " 'docs-main/images/metrics/descriptors_tests-min.png',\n",
       " 'docs-main/images/metrics/preset_classification-min.png',\n",
       " 'docs-main/images/metrics/preset_classification_2-min.png',\n",
       " 'docs-main/images/metrics/preset_classification_example-min.png',\n",
       " 'docs-main/images/metrics/preset_data_drift-min.png',\n",
       " 'docs-main/images/metrics/preset_data_drift_2-min.png',\n",
       " 'docs-main/images/metrics/preset_data_drift_3-min.png',\n",
       " 'docs-main/images/metrics/preset_dataset_summary-min.png',\n",
       " 'docs-main/images/metrics/preset_datasummary_example-min.png',\n",
       " 'docs-main/images/metrics/preset_recsys-min.png',\n",
       " 'docs-main/images/metrics/preset_recsys_2-min.png',\n",
       " 'docs-main/images/metrics/preset_regression-min.png',\n",
       " 'docs-main/images/metrics/preset_regression_2-min.png',\n",
       " 'docs-main/images/metrics/preset_text_evals-min.gif',\n",
       " 'docs-main/images/metrics/preset_value_stats-min.png',\n",
       " 'docs-main/images/metrics/readme.md',\n",
       " 'docs-main/images/metrics/test_preset_classification-min.png',\n",
       " 'docs-main/images/metrics/test_preset_data_drift-min.png',\n",
       " 'docs-main/images/metrics/test_preset_dataset_summary-min.png',\n",
       " 'docs-main/images/metrics/test_preset_recsys-min.png',\n",
       " 'docs-main/images/metrics/test_preset_regression-min.png',\n",
       " 'docs-main/images/monitoring_batch_workflow_min.png',\n",
       " 'docs-main/images/monitoring_flow_batch.png',\n",
       " 'docs-main/images/monitoring_flow_tracing.png',\n",
       " 'docs-main/images/nocode_choose_evals-min.png',\n",
       " 'docs-main/images/nocode_column_mapping-min.png',\n",
       " 'docs-main/images/nocode_includes_words-min.png',\n",
       " 'docs-main/images/nocode_judge_result-min.png',\n",
       " 'docs-main/images/nocode_llm_judge-min.png',\n",
       " 'docs-main/images/nocode_semantic_similarity-min.png',\n",
       " 'docs-main/images/nocode_start_eval-min.png',\n",
       " 'docs-main/images/platform_compare_select.png',\n",
       " 'docs-main/images/platform_compare_view.png',\n",
       " 'docs-main/images/platform_small-min.png',\n",
       " 'docs-main/images/projects.png',\n",
       " 'docs-main/images/synth_data-min.png',\n",
       " 'docs-main/images/synthetic/',\n",
       " 'docs-main/images/synthetic/datagen_travel.gif',\n",
       " 'docs-main/images/synthetic/readme.md',\n",
       " 'docs-main/images/synthetic/synthetic_adversarial_img.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_adversarial.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_brand_image.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_forbidden.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_inputs_example_prompt.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_inputs_example_result.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_inputs_example_upload.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_inputs_example_upload2.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_rag_example_result.png',\n",
       " 'docs-main/images/synthetic/synthetic_data_select_method.png',\n",
       " 'docs-main/images/synthetic/synthetic_experiments_img.png',\n",
       " 'docs-main/images/test_suite_dashboard-min.png',\n",
       " 'docs-main/images/tracing.png',\n",
       " 'docs-main/introduction.mdx',\n",
       " 'docs-main/logo/',\n",
       " 'docs-main/logo/evidently_ai_logo_docs.png',\n",
       " 'docs-main/logo/evidently_ai_logo_docs.svg',\n",
       " 'docs-main/logo/evidently_ai_logo_docs_dark.png',\n",
       " 'docs-main/logo/evidently_ai_logo_docs_dark.svg',\n",
       " 'docs-main/logo/favicon.png',\n",
       " 'docs-main/logo/favicon.svg',\n",
       " 'docs-main/main_img',\n",
       " 'docs-main/metrics/',\n",
       " 'docs-main/metrics/all_descriptors.mdx',\n",
       " 'docs-main/metrics/all_metrics.mdx',\n",
       " 'docs-main/metrics/all_presets.mdx',\n",
       " 'docs-main/metrics/customize_add_text.mdx',\n",
       " 'docs-main/metrics/customize_colors.mdx',\n",
       " 'docs-main/metrics/customize_data_drift.mdx',\n",
       " 'docs-main/metrics/customize_descriptor.mdx',\n",
       " 'docs-main/metrics/customize_embedding_drift.mdx',\n",
       " 'docs-main/metrics/customize_hf_descriptor.mdx',\n",
       " 'docs-main/metrics/customize_llm_judge.mdx',\n",
       " 'docs-main/metrics/customize_metric.mdx',\n",
       " 'docs-main/metrics/explainer_classification.mdx',\n",
       " 'docs-main/metrics/explainer_data_stats.mdx',\n",
       " 'docs-main/metrics/explainer_drift.mdx',\n",
       " 'docs-main/metrics/explainer_llm_evals.mdx',\n",
       " 'docs-main/metrics/explainer_recsys.mdx',\n",
       " 'docs-main/metrics/explainer_regression.mdx',\n",
       " 'docs-main/metrics/introduction.mdx',\n",
       " 'docs-main/metrics/preset_classification.mdx',\n",
       " 'docs-main/metrics/preset_data_drift.mdx',\n",
       " 'docs-main/metrics/preset_data_summary.mdx',\n",
       " 'docs-main/metrics/preset_recsys.mdx',\n",
       " 'docs-main/metrics/preset_regression.mdx',\n",
       " 'docs-main/metrics/preset_text_evals.mdx',\n",
       " 'docs-main/mint.json',\n",
       " 'docs-main/quickstart_llm.mdx',\n",
       " 'docs-main/quickstart_ml.mdx',\n",
       " 'docs-main/quickstart_tracing.mdx',\n",
       " 'docs-main/snippets/',\n",
       " 'docs-main/snippets/cloud_signup.mdx',\n",
       " 'docs-main/snippets/create_project.mdx',\n",
       " 'docs-main/synthetic-data/',\n",
       " 'docs-main/synthetic-data/adversarial_data.mdx',\n",
       " 'docs-main/synthetic-data/input_data.mdx',\n",
       " 'docs-main/synthetic-data/introduction.mdx',\n",
       " 'docs-main/synthetic-data/rag_data.mdx',\n",
       " 'docs-main/synthetic-data/why_synthetic.mdx']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c6dd9d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = {\n",
    "    'content': post.content,\n",
    "    'metadata': post.metadata.get('title'),\n",
    "    'description': post.metadata.get('description'),\n",
    "    'filename': filename_corrected\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e2ed975a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'content': '<Check>\\n  Built-in alerting is a Pro feature available in the **Evidently Cloud** and **Evidently Enterprise**.\\n</Check>\\n\\n![](/images/alerts.png)\\n\\nTo enable alerts, open the Project and navigate to the \"Alerts\" in the left menu. You must set:\\n\\n* A notification channel.\\n\\n* An alert condition.\\n\\n## Notification channels\\n\\nYou can choose between the following options:\\n\\n* **Email**. Add email addresses to send alerts to.\\n\\n* **Slack**. Add a Slack webhook.\\n\\n* **Discord**. Add a Discord webhook.\\n\\n## Alert conditions\\n\\n### Failed tests\\n\\nIf you use Tests (conditional checks) in your Project, you can tie alerting to the failed Tests in a Test Suite. Toggle this option on the Alerts page. Evidently will set an alert to the defined channel if any of the Tests fail.\\n\\n<Tip>\\n  **How to avoid alert fatigue?** Use the `is_critical` parameter to mark non-critical Test as Warnings. Setting it to `False` prevent alerts for those checks even if they fail.\\n</Tip>\\n\\n### Custom conditions\\n\\nYou can also set alerts on individual Metric values. For example, you can generate Alerts when the share of drifting features is above a certain threshold.\\n\\nClick on the plus sign below the “Add new Metric alert” and follow the prompts to set an alert condition.\\n\\n![](../.gitbook/assets/cloud/alerts.png)',\n",
       " 'metadata': 'Alerts',\n",
       " 'description': 'How to set up alerts.',\n",
       " 'filename': 'docs/platform/alerts.mdx'}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2d43cef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_github_repository(repo_owner, repo_name, branch=\"main\"):\n",
    "    url = f\"https://github.com/{repo_owner}/{repo_name}/archive/refs/heads/{branch}.zip\"\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    documents = []\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as zip_ref:\n",
    "        for file_path in zip_ref.namelist():\n",
    "            if not file_path.endswith(('.md', '.mdx')):\n",
    "                continue\n",
    "            with zip_ref.open(file_path) as file:\n",
    "                content = file.read().decode('utf-8')\n",
    "                post = frontmatter.loads(content)\n",
    "                doc = {\n",
    "                    'content': post.content,\n",
    "                    'title': post.metadata.get('title'),\n",
    "                    'description': post.metadata.get('description'),\n",
    "                    'filename': file_path.split('/', 1)[-1]\n",
    "                }\n",
    "                documents.append(doc)\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f76b3f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_owner = 'evidentlyai'\n",
    "repo_name = 'docs'\n",
    "branch_name = 'main'\n",
    "\n",
    "documents = read_github_repository(repo_owner=repo_owner, repo_name=repo_name, branch=branch_name)\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "02c04132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 95 documents.\n"
     ]
    }
   ],
   "source": [
    "from gitsource import GithubRepositoryDataReader\n",
    "\n",
    "reader = GithubRepositoryDataReader(\n",
    "    repo_owner='evidentlyai',\n",
    "    repo_name = 'docs',\n",
    "    allowed_extensions={'md', 'mdx'},\n",
    ")\n",
    "\n",
    "files = reader.read()\n",
    "\n",
    "print(f'Loaded {len(files)} documents.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cb0d38c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file = files[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "405e99f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Output formats',\n",
       " 'description': 'How to export the evaluation results.',\n",
       " 'content': 'You can view or export Reports in multiple formats.\\n\\n**Pre-requisites**:\\n\\n* You know how to [generate Reports](/docs/library/report).\\n\\n## Log to Workspace\\n\\nYou can save the computed Report in Evidently Cloud or your local workspace.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=False)\\n```\\n\\n<Info>\\n  **Uploading evals**. Check Quickstart examples [for ML](/quickstart_ml) or [for LLM](/quickstart_llm) for a full workflow.\\n</Info>\\n\\n## View in Jupyter notebook\\n\\nYou can directly render the visual summary of evaluation results in interactive Python environments like Jupyter notebook or Colab.\\n\\nAfter running the Report, simply call the resulting Python object:\\n\\n```python\\nmy_report\\n```\\n\\nThis will render the HTML object directly in the notebook cell.\\n\\n## HTML\\n\\nYou can also save this interactive visual Report as an HTML file to open in a browser:\\n\\n```python\\nmy_report.save_html(“file.html”)\\n```\\n\\nThis option is useful for sharing Reports with others or if you\\'re working in a Python environment that doesn’t display interactive visuals.\\n\\n## JSON\\n\\nYou can get the results of the calculation as a JSON. It is useful for storing and exporting results elsewhere.\\n\\nTo view the JSON in Python:\\n\\n```python\\nmy_report.json()\\n```\\n\\nTo save the JSON as a separate file:\\n\\n```python\\nmy_report.save_json(\"file.json\")\\n```\\n\\n## Python dictionary\\n\\nYou can get the output as a Python dictionary. This format is convenient for automated evaluations in data or ML pipelines, allowing you to transform the output or extract specific values.\\n\\nTo get the dictionary:\\n\\n```python\\nmy_report.dict()\\n```',\n",
       " 'filename': 'docs/library/output_formats.mdx'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_file.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ad498af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [f.parse() for f in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d6089325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e029db",
   "metadata": {},
   "source": [
    "# Indexing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e30a87",
   "metadata": {},
   "source": [
    "1. search <-- elastic search or minsearch\n",
    "2. prompt \n",
    "3. llm>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bab9401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "1cbd3f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'LLM as a Judge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "73659028",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x783df61b5e20>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = Index(\n",
    "    text_fields=['title', 'description', 'content'],\n",
    "    keyword_fields = ['filename']\n",
    ")\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d1d3edf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9c0baf21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0c5fd7dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'LLM as a judge',\n",
       " 'description': 'How to create and evaluate an LLM judge.',\n",
       " 'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quickly and easily.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I add another user to my account?\",\\n     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\\n     \"To add a secondary user, go to \\'Account Settings\\', select \\'Manage Users\\', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\\n     \"incorrect\", \"contradiction (incorrect answer)\"],\\n  \\n    [\"Is it possible to link multiple bank accounts?\",\\n     \"Yes, you can link multiple bank accounts by going to \\'Account Settings\\' in the menu and selecting \\'Add Bank Account\\'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\\n     \"You can add multiple bank accounts by visiting \\'Accounts\\' in the menu and choosing \\'Add Bank Account\\'. Enter your bank details as prompted and complete the verification process for each account to link them successfully.\",\\n     \"incorrect\", \"contradiction (incorrect menu item)\"],\\n  \\n    [\"Can I use your service for cryptocurrency transactions?\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"Currently, our service does not support cryptocurrency transactions. Please check our website for updates regarding this feature. We are continuously working to expand our services and may include this in the future.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Hi, can I get a detailed report of my monthly transactions?\",\\n     \"Yes, you can generate a detailed monthly report of your transactions by logging into your account, going to \\'Transaction History\\', and selecting \\'Generate Report\\'. You can customize the report by selecting specific dates or transaction types.\",\\n     \"You can get a detailed monthly report by logging into your account, navigating to \\'Transaction History\\', and clicking on \\'Generate Report\\'. Customize your report by choosing the date range and types of transactions you want to include.\",\\n     \"correct\", \"\"],\\n  \\n    [\"I am traveling to the US. Can I use the app there?\",\\n     \"Yes, you can use the app in the US just like you do at home. Ensure you have an internet connection. You may also want to update your app to the latest version before traveling for optimal performance.\",\\n     \"The app will work in the US without any issues. Just make sure you have access to the internet. For the best experience, update your app to the latest version before you travel.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I link my payment account to a new mobile number?\",\\n     \"To link a new mobile number, log in to your account, go to \\'Account Settings\\', select \\'Mobile Number\\', and follow the instructions to verify your new number. You will need to enter the new number and verify it via a code sent to your phone.\",\\n     \"To add a new number, navigate to the \\'Account Settings\\' section, select \\'Mobile Number\\' and proceed with the steps to add and confirm the new number. Enter the new mobile number and verify it using the code sent to your phone.\",\\n     \"correct\", \"\"],\\n  \\n    [\"Can I receive notifications for transactions in real-time?\",\\n     \"Yes, you can enable real-time notifications for transactions by going to \\'Account Settings\\', then \\'Notifications\\', and turning on \\'Transaction Alerts\\'. You can choose to receive alerts via SMS, email, or push notifications on your mobile device.\",\\n     \"To receive real-time notifications for transactions, log into your account, go to \\'Account Settings\\', select \\'Notifications\\', and enable \\'Transaction Alerts\\'. Choose your preferred notification method between email or push notifications.\",\\n     \"incorrect\", \"omits information (sms notification)\"],\\n  \\n    [\"Hey, can I set up automatic transfers to my savings account?\",\\n     \"Yes, you can set up automatic transfers by going to \\'Account Settings\\', selecting \\'Automatic Transfers\\', and specifying the amount and frequency. You can choose to transfer weekly, bi-weekly, or monthly. Make sure to save the settings to activate the transfers.\",\\n     \"You can arrange automatic transfers by going to \\'Account Settings\\', choosing \\'Automatic Transfers\\', and setting the desired amount and frequency. Don\\'t forget to save the changes to enable the automatic transfers.\",\\n     \"incorrect\", \"omits information (limited frequency of transfers available)\"],\\n  \\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How can I update my billing address?\",\\n     \"To update your billing address, log into your account, go to \\'Account Settings\\', select \\'Billing Information\\', and enter your new address. Make sure to save the changes once you are done.\",\\n     \"To update your billing address, log into your account, navigate to \\'Account Settings\\', and select \\'Billing Information\\'. Enter your new address and ensure all fields are filled out correctly. Save the changes, and you will receive a confirmation email with the updated address details.\",\\n     \"incorrect\", \"adds new information (confirmation email)\"],\\n  \\n    [\"How do I contact customer support?\",\\n     \"You can contact customer support by logging into your account, going to the \\'Help\\' section, and selecting \\'Contact Us\\'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\\n     \"To contact customer support, log into your account and go to the \\'Help\\' section. Select \\'Contact Us\\' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\\n     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\\n  \\n    [\"What should I do if my card is lost or stolen?\",\\n     \"If your card is lost or stolen, immediately log into your account, go to \\'Card Management\\', and select \\'Report Lost/Stolen\\'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\\n     \"If your card is lost or stolen, navigate to \\'Card Management\\' in your account, and select \\'Report Lost/Stolen\\'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I enable two-factor authentication (2FA)?\",\\n     \"To enable two-factor authentication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).',\n",
       " 'filename': 'examples/LLM_judge.mdx'}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Best match\n",
    "results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09aa42a4",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "826a85bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "metrics/all_metrics.mdx: 55085 characters\n",
      "metrics/all_descriptors.mdx: 31976 characters\n",
      "docs/platform/dashboard_panel_types.mdx: 31647 characters\n",
      "docs/library/leftover_content.mdx: 28742 characters\n",
      "metrics/customize_llm_judge.mdx: 26847 characters\n"
     ]
    }
   ],
   "source": [
    "doc_sizes = [(doc.filename, len(doc.content)) for doc in files]\n",
    "doc_sizes.sort(key = lambda x: x[1], reverse=True)\n",
    "\n",
    "for filename, size in doc_sizes[:5]:\n",
    "    print(f'{filename}: {size} characters')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506a625a",
   "metadata": {},
   "source": [
    "1. search <-- 5 docs\n",
    "2. prompt <-- 5 X 20k = 100k characters (prices for tokens)\n",
    "3. llm>>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "616df775",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = list(range(0,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ff7cb865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      "[5, 6, 7, 8, 9, 10, 11, 12, 13, 14]\n",
      "[10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
      "[15, 16, 17, 18, 19, 20, 21, 22, 23, 24]\n",
      "[20, 21, 22, 23, 24, 25, 26, 27, 28, 29]\n",
      "[25, 26, 27, 28, 29, 30, 31, 32, 33, 34]\n",
      "[30, 31, 32, 33, 34, 35, 36, 37, 38, 39]\n",
      "[35, 36, 37, 38, 39, 40, 41, 42, 43, 44]\n",
      "[40, 41, 42, 43, 44, 45, 46, 47, 48, 49]\n",
      "[45, 46, 47, 48, 49, 50, 51, 52, 53, 54]\n",
      "[50, 51, 52, 53, 54, 55, 56, 57, 58, 59]\n",
      "[55, 56, 57, 58, 59, 60, 61, 62, 63, 64]\n",
      "[60, 61, 62, 63, 64, 65, 66, 67, 68, 69]\n",
      "[65, 66, 67, 68, 69, 70, 71, 72, 73, 74]\n",
      "[70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n",
      "[75, 76, 77, 78, 79, 80, 81, 82, 83, 84]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89]\n",
      "[85, 86, 87, 88, 89, 90, 91, 92, 93, 94]\n",
      "[90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n"
     ]
    }
   ],
   "source": [
    "window_size = 10\n",
    "start = 0\n",
    "step = 5\n",
    "\n",
    "chunks = []\n",
    "\n",
    "while start < len(documents):\n",
    "    end = start + window_size\n",
    "    chunk = documents[start:end]\n",
    "    if len(chunk) < window_size:\n",
    "        break\n",
    "    chunks.append(chunks)\n",
    "    print(chunk)\n",
    "    start = start + step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25c7a3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window(text, size=1000, step=500):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    text_length = len(text)\n",
    "\n",
    "    while start < text_length:\n",
    "        end = start + size\n",
    "        chunk = text[start:end]\n",
    "        chunks.append({'start': start, 'content': chunk})\n",
    "\n",
    "        start = end - step\n",
    "\n",
    "        if end >= text_length:\n",
    "            break\n",
    "\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a917ccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sliding_window(results[0]['content'], size=3000, step=2500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "132305b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = []\n",
    "\n",
    "for doc in documents:\n",
    "    if not doc.get('content'):\n",
    "        continue\n",
    "    copy = doc.copy()\n",
    "    content = copy.pop('content')\n",
    "\n",
    "    chunks = sliding_window(content, size=3000, step=1500)\n",
    "\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.update(copy)\n",
    "        chunk['chunk_id'] = i\n",
    "        document_chunks.append(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "24021a90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'content': '<Note>\\n  If you\\'re not looking to build API reference documentation, you can delete\\n  this section by removing the api-reference folder.\\n</Note>\\n\\n## Welcome\\n\\nThere are two ways to build API documentation: [OpenAPI](https://mintlify.com/docs/api-playground/openapi/setup) and [MDX components](https://mintlify.com/docs/api-playground/mdx/configuration). For the starter kit, we are using the following OpenAPI specification.\\n\\n<Card\\n  title=\"Plant Store Endpoints\"\\n  icon=\"leaf\"\\n  href=\"https://github.com/mintlify/starter/blob/main/api-reference/openapi.json\"\\n>\\n  View the OpenAPI specification file\\n</Card>\\n\\n## Authentication\\n\\nAll API endpoints are authenticated using Bearer tokens and picked up from the specification file.\\n\\n```json\\n\"security\": [\\n  {\\n    \"bearerAuth\": []\\n  }\\n]\\n```',\n",
       "  'title': 'Introduction',\n",
       "  'description': 'Example section for showcasing API endpoints',\n",
       "  'filename': 'api-reference/introduction.mdx',\n",
       "  'chunk_id': 0},\n",
       " {'start': 0,\n",
       "  'content': '<Update label=\"2025-07-18\" description=\"Evidently v0.7.11\">\\n  ## **Evidently 0.7.11**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.11).\\n\\nExample notebooks:\\n- Synthetic data generation: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/datagen.ipynb)\\n\\n</Update>\\n\\n<Update label=\"2025-07-09\" description=\"Evidently v0.7.10\">\\n  ## **Evidently 0.7.10**\\n    Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.10).\\n  \\nNEW: automated prompt optimization. Read the release blog on [prompt optimization for LLM judges](https://www.evidentlyai.com/blog/llm-judge-prompt-optimization).\\n\\nExample notebooks:\\n- Code review binary LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_code_review_example.ipynb)\\n- Topic multi-class LLM judge prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_bookings_example.ipynb)\\n- Tweet generation prompt optimization: [code example](https://github.com/evidentlyai/evidently/blob/main/examples/cookbook/prompt_optimization_tweet_generation_example.ipynb)\\n</Update>\\n\\n<Update label=\"2025-06-27\" description=\"Evidently v0.7.9\">\\n  ## **Evidently 0.7.9**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.9).\\n</Update>\\n\\n<Update label=\"2025-06-19\" description=\"Evidently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.6).\\n</Update>\\n\\n<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\\n  ## **Evidently 0.7.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5).\\n</Update>\\n\\n<Update label=\"2025-05-05\" description=\"Evidently v0.7.4\">\\n  ## **Evidently 0.7.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4).\\n</Update>\\n\\n<Update label=\"2025-04-25\" description=\"Evidently v0.7.3\">\\n  ## **Evidently 0.7.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3).\\n</Update>\\n\\n<Update label=\"2025-04-22\" description=\"Evidently v0.7.2\">\\n  ## **Evidently 0.7.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2).\\n</Update>\\n\\n<Update label=\"2025-04-21\" description=\"Evidently v0.7.1\">\\n  ## **Evidently 0.7.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1).\\n</Update>\\n\\n\\n<Update',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'changelog/changelog.mdx',\n",
       "  'chunk_id': 0},\n",
       " {'start': 1500,\n",
       "  'content': 'ently v0.7.8\">\\n  ## **Evidently 0.7.8**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.8).\\n</Update>\\n\\n<Update label=\"2025-06-04\" description=\"Evidently v0.7.7\">\\n  ## **Evidently 0.7.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.7).\\n</Update>\\n\\n<Update label=\"2025-05-25\" description=\"Evidently v0.7.6\">\\n  ## **Evidently 0.7.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.6).\\n</Update>\\n\\n<Update label=\"2025-05-09\" description=\"Evidently v0.7.5\">\\n  ## **Evidently 0.7.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.5).\\n</Update>\\n\\n<Update label=\"2025-05-05\" description=\"Evidently v0.7.4\">\\n  ## **Evidently 0.7.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.4).\\n</Update>\\n\\n<Update label=\"2025-04-25\" description=\"Evidently v0.7.3\">\\n  ## **Evidently 0.7.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.3).\\n</Update>\\n\\n<Update label=\"2025-04-22\" description=\"Evidently v0.7.2\">\\n  ## **Evidently 0.7.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.2).\\n</Update>\\n\\n<Update label=\"2025-04-21\" description=\"Evidently v0.7.1\">\\n  ## **Evidently 0.7.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.1).\\n</Update>\\n\\n\\n<Update label=\"2025-04-10\" description=\"Evidently v7.0\">\\n  ## **Evidently 0.7**\\n\\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2).\\n</Update>\\n\\n<Update label=\"2025-03-31\" description=\"Evidently v0.6.7\">\\n  ## **Evidently 0.6.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7).\\n</Update>\\n\\n<Update label=\"2025-03-12\" description=\"Evidently v0.6.6\">\\n  ## **Evidently 0.6.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.5\">\\n  ## **Evidently 0.6.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.4\">\\n  ## **Evidently 0.6.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4).\\n</Update>\\n\\n<Update label=\"2025-02-12\" description=\"Evidently v0.6.3\">\\n  ## **Evidently 0.6.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-to',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'changelog/changelog.mdx',\n",
       "  'chunk_id': 1},\n",
       " {'start': 3000,\n",
       "  'content': ' label=\"2025-04-10\" description=\"Evidently v7.0\">\\n  ## **Evidently 0.7**\\n\\nThis release introduces breaking changes. Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.7.0).\\n* The new Evidently API becomes the default. Read the [Migration guide](/faq/migration).\\n* New Evidently Cloud version released. Read the [Evidently Cloud v2 notice](/faq/cloud_v2).\\n</Update>\\n\\n<Update label=\"2025-03-31\" description=\"Evidently v0.6.7\">\\n  ## **Evidently 0.6.7**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.7).\\n</Update>\\n\\n<Update label=\"2025-03-12\" description=\"Evidently v0.6.6\">\\n  ## **Evidently 0.6.6**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.6).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.5\">\\n  ## **Evidently 0.6.5**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.5).\\n</Update>\\n\\n<Update label=\"2025-02-17\" description=\"Evidently v0.6.4\">\\n  ## **Evidently 0.6.4**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.4).\\n</Update>\\n\\n<Update label=\"2025-02-12\" description=\"Evidently v0.6.3\">\\n  ## **Evidently 0.6.3**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.3). Added new RAG descriptors: see [tutorial](/examples/LLM_rag_evals) and [release blog](https://www.evidentlyai.com/blog/open-source-rag-evaluation-tool).\\n</Update>\\n\\n<Update label=\"2025-02-07\" description=\"Evidently v0.6.2\">\\n  ## **Evidently 0.6.2**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.2). We extended support for `litellm` , so you can easily use different providers like Gemini, Anthropic, etc. for LLM-based evaluations.\\n</Update>\\n\\n<Update label=\"2025-01-31\" description=\"Evidently v0.6.1\">\\n  ## **Evidently 0.6.1**\\n\\n  Full release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.1).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently v0.6\">\\n  ## **New API release**\\n\\n  The new API is available when you import modules from `evidently.future`. Read more in [Migration guide](/faq/migration). Release notes on [Github](https://github.com/evidentlyai/evidently/releases/tag/v0.6.0).\\n</Update>\\n\\n<Update label=\"2025-01-24\" description=\"Evidently Cloud\">\\n  ## **Editable datasets**\\n\\n  You can now hit \"edit\" on any existing dataset, create a copy and add / delete rows and columns. Use it while working on your evaluation datasets or to leave comments on outputs.\\n\\n  ![](/images/changelog/editable_dataset-min.png)\\n</Update>\\n\\n<Update label=\"2025-01-10\" description=\"Docs\">\\n  ## **New Docs**\\n\\n  We are creating a new Docs website in anticipation of API change. You can still access old docs for information on earlier API and examples.\\n</Update>',\n",
       "  'title': 'Product updates',\n",
       "  'description': 'Latest releases.',\n",
       "  'filename': 'changelog/changelog.mdx',\n",
       "  'chunk_id': 2},\n",
       " {'start': 0,\n",
       "  'content': 'To run evaluations, you must create a `Dataset` object with a `DataDefinition`, which maps:\\n\\n- **Column types** (e.g., categorical, numerical, text).\\n- **Column roles** (e.g., id, prediction, target).\\n\\nThis allows Evidently to process the data correctly. Some evaluations need specific columns and will fail if they\\'re missing. You can define the mapping using the Python API or by assigning columns visually when uploading data to the Evidently platform.\\n\\n## Basic flow\\n\\n**Step 1. Imports.** Import the following modules:\\n\\n```python\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\n```\\n\\n**Step 2. Prepare your data.** Use a pandas.DataFrame.\\n\\n<Info>\\n  Your data can have [flexible structure](/docs/library/overview#dataset) with any mix of categorical, numerical or text columns. Check the [Reference table](/metrics/all_metrics) for data requirements in specific evaluations.\\n</Info>\\n\\n**Step 3. Create a Dataset object**. Use `Dataset.from_pandas` with `data_definition`:\\n\\n```python\\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=DataDefinition()\\n)\\n```\\n\\nTo map columns automatically, pass an empty `DataDefinition()` . Evidently will map columns:\\n\\n- By type (numerical, categorical).\\n- By matching column names to roles (e.g., a column \"target\" treated as target).\\n\\nAutomation works in many cases, but manual mapping is more accurate. It is also necessary for evaluating prediction quality or handling text columns.\\n\\n<Note>\\n  **How to set the data definition manually?** See the section below for available options.\\n</Note>\\n\\n**Step 4. Run evals.** Once the **Dataset** object is ready, you can [add Descriptors ](/docs/library/descriptors) and[ run Reports](/docs/library/report).\\n\\n### Special cases\\n\\n**Working directly with pandas.DataFrame**. You can sometimes pass a `pandas.DataFrame` directly to `report.run()` without creating the Dataset object. This works for checks like numerical/categorical data summaries or drift detection. However, it\\'s best to always create a `Dataset` object explicitly for clarity and control.\\n\\n**Working with two datasets**. If you\\'re working with current and reference datasets (e.g., for drift detection), create a Dataset object for each. Both must have identical data definition.\\n\\n## Data definition\\n\\nThis page shows all the different mapping options. Note that you **only need to use the relevant ones** that apply for your evaluation scenario. For example, you don’t need columns like target/prediction to run data quality or LLM checks.\\n\\n### Column types\\n\\nKnowing the column type helps compute correct statistics, visualizations, and pick default tests.\\n\\n#### Text data\\n\\nIf you run LLM evaluations, simply specify the columns with inputs/outputs as text.\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"Latest_Review\"]\\n    )\\n    \\neval_data = Dataset.from_pandas(\\n    source_df,\\n    data_definition=definition\\n)\\n```\\n\\n<Info>\\n  **It\\'s optional but useful**. You can [generate text descriptors](/docs/library/d',\n",
       "  'title': 'Data definition',\n",
       "  'description': 'How to map the input data.',\n",
       "  'filename': 'docs/library/data_definition.mdx',\n",
       "  'chunk_id': 0}]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a2bd8ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.minsearch.Index at 0x783df61dd1f0>"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_index = Index(\n",
    "    text_fields=['title', 'description', 'content'],\n",
    "    keyword_fields = ['filename']\n",
    ")\n",
    "chunk_index.fit(document_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "22b3dbde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'start': 0,\n",
       "  'content': 'import CloudSignup from \\'/snippets/cloud_signup.mdx\\';\\nimport CreateProject from \\'/snippets/create_project.mdx\\';\\n\\nIn this tutorial, we\\'ll show how to evaluate text for custom criteria using LLM as the judge, and evaluate the LLM judge itself.\\n\\n<Info>\\n  **This is a local example.** You will run and explore results using the open-source Python library. At the end, we’ll optionally show how to upload results to the Evidently Platform for easy exploration.\\n</Info>\\n\\nWe\\'ll explore two ways to use an LLM as a judge:\\n\\n- **Reference-based**. Compare new responses against a reference. This is useful for regression testing or whenever you have a \"ground truth\" (approved responses) to compare against.\\n- **Open-ended**. Evaluate responses based on custom criteria, which helps evaluate new outputs when there\\'s no reference available.\\n\\nWe will focus on demonstrating **how to create and tune the LLM evaluator**, which you can then apply in different contexts, like regression testing or prompt comparison.\\n\\n<Info>\\n**Prefer videos?** We also have an extended code tutorial where we iteratively improve the prompt for LLM judge with a video walkthrough:  https://www.youtube.com/watch?v=kP_aaFnXLmY\\n</Info>\\n\\n## Tutorial scope\\n\\nHere\\'s what we\\'ll do:\\n\\n- **Create an evaluation dataset**. Create a toy Q&A dataset.\\n- **Create and run an LLM as a judge**. Design an LLM evaluator prompt.\\n- **Evaluate the judge**. Compare the LLM judge\\'s evaluations with manual labels.\\n\\nWe\\'ll start with the reference-based evaluator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metric',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 0},\n",
       " {'start': 1500,\n",
       "  'content': 'valuator that determines whether a new response is correct (it\\'s more complex since it requires passing two columns to the prompt). Then, we\\'ll create a simpler judge focused on verbosity.\\n\\nTo complete the tutorial, you will need:\\n\\n- Basic Python knowledge.\\n- An OpenAI API key to use for the LLM evaluator.\\n\\nWe recommend running this tutorial in Jupyter Notebook or Google Colab to render rich HTML objects with summary results directly in a notebook cell.\\n\\n<Info>\\n  Run a sample notebook: [Jupyter notebook](https://github.com/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb) or [open it in Colab](https://colab.research.google.com/github/evidentlyai/community-examples/blob/main/tutorials/LLM_as_a_judge_tutorial_updated.ipynb).\\n</Info>\\n\\n## 1.  Installation and Imports\\n\\nInstall Evidently:\\n\\n```python\\npip install evidently\\n```\\n\\nImport the required modules:\\n\\n```python\\nimport pandas as pd\\nimport numpy as np\\n\\nfrom evidently import Dataset\\nfrom evidently import DataDefinition\\nfrom evidently import Report\\nfrom evidently import BinaryClassification\\nfrom evidently.descriptors import *\\nfrom evidently.presets import TextEvals, ValueStats, ClassificationPreset\\nfrom evidently.metrics import *\\n\\nfrom evidently.llm.templates import BinaryClassificationPromptTemplate\\n```\\n\\nPass your OpenAI key as an environment variable:\\n\\n```python\\nimport os\\nos.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n```\\n\\n<Info>\\n**Using other evaluator LLMs**. Check the [LLM judge docs](/metrics/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 1},\n",
       " {'start': 19500,\n",
       "  'content': 'he message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically track the results, and interact with your evaluation dataset, you can use the Evidently Cloud platform.\\n\\n### Set up Evidently Cloud\\n\\n<CloudSignup />\\n\\nImport the components to connect with Evidently Cloud:\\n\\n```python\\nfrom evidently.ui.workspace import CloudWorkspace\\n```\\n\\n### Create a Project\\n\\n<CreateProject />\\n\\n### Send your eval\\n\\nSince you already created the eval, you can simply upload it to the Evidently Cloud.\\n\\n```python\\nws.add_run(project.id, my_eval, include_data=True)\\n```\\n\\nYou can then go to the Evidently Cloud, open your Project and explore the Report.\\n\\n![](/images/examples/llm_judge_tutorial_cloud-min.png)\\n\\n<Info>\\n  You can also [create the LLM judges with no-code](/docs/platform/evals_no_code).\\n</Info>\\n\\n# Reference documentation\\n\\nSee this page for complete [documentation on LLM judges](/metrics/customize_llm_judge).',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 13},\n",
       " {'start': 15000,\n",
       "  'content': ' REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFra',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 10},\n",
       " {'start': 16500,\n",
       "  'content': 'ptors(descriptors=[\\n    ExactMatch(columns=[\"label\", \"Correctness\"], alias=\"Judge_match\")])\\n```\\n\\n**Get a Report.** Summarize the result by generating an Evidently Report.\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\nThis will render an HTML report in the notebook cell. You can use other [export options](/docs/library/output_formats), like `as_dict()` for a Python dictionary output.\\n\\n![](/images/examples/llm_judge_tutorial_report-min.png)\\n\\nSince we already performed exact matching, you can see the crude accuracy of our judge. However, accuracy is not always the best metric. In this case, we might be more interested in recall: we want to make sure that the judge does not miss any \"incorrect\" answers .\\n\\n## 4. Evaluate the LLM Eval quality\\n\\nThis part is a bit meta: we\\'re going to evaluate the quality of our LLM evaluator itself\\\\! We can treat it as a simple **binary classification** problem.\\n\\n**Data definition**. To evaluate the classification quality, we need to map the structure of the dataset accordingly first. The column with the manual label is the \"target\", and the LLM-judge response is the \"prediction\":\\n\\n```python\\ndf=eval_dataset.as_dataframe()\\n\\ndefinition_2 = DataDefinition(\\n    classification=[BinaryClassification(\\n        target=\"label\",\\n        prediction_labels=\"Correctness\",\\n        pos_label = \"incorrect\")],\\n    categorical_columns=[\"label\", \"Correctness\"])\\n\\nclass_dataset = Dataset.from_pandas(\\n    pd.DataFrame(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey t',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 11},\n",
       " {'start': 18000,\n",
       "  'content': 'me(df),\\n    data_definition=definition_2)\\n```\\n\\n<Info>\\n  `Pos_label` refers to the class that is treated as the target (\"what we want to predict better\") for metrics like precision, recall, F1-score.\\n</Info>\\n\\n**Get a Report**. Let\\'s use a`ClassificationPreset()` that combines several classification metrics:\\n\\n```python\\nreport = Report([\\n    ClassificationPreset()\\n])\\n\\nmy_eval = report.run(class_dataset, None)\\nmy_eval\\n\\n# or my_eval.as_dict()\\n```\\n\\nWe can now get a well-rounded evaluation and explore the confusion matrix. We have one type of error each: overall the results are pretty good\\\\! You can also refine the prompt to try to improve them.\\n\\n![](/images/examples/llm_judge_tutorial_conf_matrix-min.png)\\n\\n## 5. Verbosity evaluator\\n\\nNext, let’s create a simpler verbosity judge. It will check whether the responses are concise and to the point. This only requires evaluating one output column: such checks are perfect for production evaluations where you don’t have a reference answer.\\n\\nHere\\'s how to set up the prompt template for verbosity:\\n\\n```python\\nverbosity = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"Conciseness refers to the quality of being brief and to the point, while still providing all necessary information.\\n            A concise response should:\\n            - Provide the necessary information without unnecessary details or repetition.\\n            - Be brief yet comprehensive enough to address the query.\\n            - Use simple and direct language to convey the message effectively.\"\"\",\\n        target_category=\"concise\",\\n        non_target_category=\"verbose\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert text evaluator. You will be given a text of the response to a user question.\")],\\n        )\\n```\\n\\nAdd this new descriptor to our existing dataset:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=verbosity,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Verbosity\")\\n    ])\\n```\\n\\nRun the Report and view the summary results:\\xa0\\n\\n```python\\nreport = Report([\\n    TextEvals()\\n])\\n\\nmy_eval = report.run(eval_dataset, None)\\nmy_eval\\n```\\n\\n![](/images/examples/llm_judge_tutorial_verbosity-min.png)\\n\\nYou can also view the dataframe using `eval_dataset.as_dataframe()`\\n\\n<Info>\\n  Don\\'t fully agree with the results? Use these labels as a starting point, edit the decisions where you see fit - now you\\'ve got your golden dataset\\\\! Next, iterate on your judge prompt. You can also try different evaluator LLMs to see which one does the job better. [How to change an LLM](/metrics/customize_llm_judge#change-the-evaluator-llm).\\n</Info>\\n\\n## What\\'s next?\\n\\nThe LLM judge itself is just one part of your overall evaluation framework. You can integrate this evaluator into different workflows, such as testing your LLM outputs after changing a prompt.\\n\\nTo be able to easily run and compare evals, systematically tr',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 12},\n",
       " {'start': 3000,\n",
       "  'content': 's/customize_llm_judge#change-the-evaluator-llm) to see how you can select a different evaluator LLM. \\n</Info>\\n\\n## 2.  Create the Dataset\\n\\nFirst, we\\'ll create a toy Q&A dataset with customer support question that includes:\\n\\n- **Questions**. The inputs sent to the LLM app.\\n- **Target responses**. The approved responses you consider accurate.\\n- **New responses**. Imitated new responses from the system.\\n- **Manual labels with explanation**. Labels that say if response is correct or not.\\n\\nWhy add the labels? It\\'s a good idea to be the judge yourself before you write a prompt. This helps:\\n\\n- Formulate better criteria. You discover nuances that help you write a better prompt.\\n- Get the \"ground truth\". You can use it to evaluate the quality of the LLM judge.\\n\\nUltimately, an LLM judge is a small ML system, and it needs its own evals\\\\!\\n\\n**Generate the dataframe**. Here\\'s how you can create this dataset in one go:\\n\\n<Accordion title=\"Toy data to run the example\" defaultOpen={false}>\\n  ```python\\n  data = [\\n    [\"Hi there, how do I reset my password?\",\\n     \"To reset your password, click on \\'Forgot Password\\' on the login page and follow the instructions sent to your registered email.\",\\n     \"To change your password, select \\'Forgot Password\\' on the login screen and follow the steps sent to your registered email address. If you don\\'t receive the email, check your spam folder or contact support for assistance.\",\\n     \"incorrect\", \"adds new information (contact support)\"],\\n  \\n    [\"Where can I find my transaction history?\",\\n     \"You can view your transaction history by logging into your account and navigating to the \\'Transaction History\\' section. Here, you can see all your past transactions. You can also filter the transactions by date or type for easier viewing.\",\\n     \"Log into your account and go to \\'Transaction History\\' to see all your past transactions. In this section, you can view and filter your transactions by date or type. This allows you to find specific transactions quickly and easily.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I add another user to my account?\",\\n     \"I am afraid it is not currently possible to add multiple users to the account. Our system supports only one user per account for security reasons. We recommend creating separate accounts for different users.\",\\n     \"To add a secondary user, go to \\'Account Settings\\', select \\'Manage Users\\', and enter the details of the person you want to add. You can set permissions for their access, deciding what they can and cannot do within the account.\",\\n     \"incorrect\", \"contradiction (incorrect answer)\"],\\n  \\n    [\"Is it possible to link multiple bank accounts?\",\\n     \"Yes, you can link multiple bank accounts by going to \\'Account Settings\\' in the menu and selecting \\'Add Bank Account\\'. Follow the prompts to add your bank account details. Make sure to verify each bank account by following the verification process.\",\\n     \"You can add multiple bank accounts by visiting \\'Accounts\\' in the menu and choosing \\'',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 2},\n",
       " {'start': 13500,\n",
       "  'content': '_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and REFERENCE\")],\\n        )\\n```\\n\\n<Info>\\n  The **Binary Classification** template (check [docs](/metrics/customize_llm_judge)) instructs an LLM to classify the input into two classes and add reasoning. You don\\'t need to ask for these details explicitly, or worry about parsing the output structure — that\\'s built into the template. You only need to add the criteria. You can also use a multi-class template.\\n</Info>\\n\\nIn this example, we\\'ve set up the prompt to be strict (\"all fact and details\"). You can write it differently. This flexibility is one of the key benefits of creating a custom judge.\\n\\n**Score your data**. To add this new descriptor to your dataset, run:\\n\\n```python\\neval_dataset.add_descriptors(descriptors=[\\n    LLMEval(\"new_response\",\\n            template=correctness,\\n            provider = \"openai\",\\n            model = \"gpt-4o-mini\",\\n            alias=\"Correctness\",\\n            additional_columns={\"target_response\": \"target_response\"}),\\n    ])\\n```\\n\\n**Preview the results**. You can view the scored dataset in Python. This will show a DataFrame with newly added scores and explanations.\\n\\n```python\\neval_dataset.as_dataframe()\\n```\\n\\n![](/images/examples/llm_judge_tutorial_judge_scored_data-min.png)\\n\\n<Info>\\n  **Note**: your explanations will vary since LLMs are non-deterministic.\\n</Info>\\n\\nIf you want, you can also add the column that will help you easily sort and find all error where the LLM-judged label is different from the ground truth label.\\n\\n```python\\neval_dataset.add_descri',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 9},\n",
       " {'start': 12000,\n",
       "  'content': 'entication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial_data_preview-min.png)\\n\\nHere\\'s the distribution of examples: we have both correct and incorrect responses.\\n\\n![](/images/examples/llm_judge_tutorial_judge_label_dist-min.png)\\n\\n<Accordion title=\"How to preview\" defaultOpen={false}>\\n  Run this to preview the distribution of the column.\\n\\n  ```python\\n  report = Report([\\n    ValueStats(column=\"label\")\\n  ])\\n  \\n  my_eval = report.run(eval_dataset, None)\\n  my_eval\\n  \\n  # my_eval.dict()\\n  # my_eval.json()\\n  ```\\n</Accordion>\\n\\n## 3. Correctness evaluator\\n\\nNow it\\'s time to set up an LLM judge! We\\'ll start with an evaluator that checks if responses are correct compared to the reference. The goal is to match the quality of our manual labels.\\n\\n**Configure the evaluator prompt**. We\\'ll use the LLMEval [Descriptor](/docs/library/descriptors) to create a custom binary evaluator. Here\\'s how to define the prompt template for correctness:\\n\\n```python\\ncorrectness = BinaryClassificationPromptTemplate(\\n        criteria = \"\"\"An ANSWER is correct when it is the same as the REFERENCE in all facts and details, even if worded differently.\\n        The ANSWER is incorrect if it contradicts the REFERENCE, adds additional claims, omits or changes details.\\n        REFERENCE:\\n        =====\\n        {target_response}\\n        =====\"\"\",\\n        target_category=\"incorrect\",\\n        non_target_category=\"correct\",\\n        uncertainty=\"unknown\",\\n        include_reasoning=True,\\n        pre_messages=[(\"system\", \"You are an expert evaluator. You will be given an ANSWER and',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 8},\n",
       " {'start': 10500,\n",
       "  'content': ' changes, and you will receive a confirmation email with the updated address details.\",\\n     \"incorrect\", \"adds new information (confirmation email)\"],\\n  \\n    [\"How do I contact customer support?\",\\n     \"You can contact customer support by logging into your account, going to the \\'Help\\' section, and selecting \\'Contact Us\\'. You can choose to reach us via email, phone, or live chat for immediate assistance.\",\\n     \"To contact customer support, log into your account and go to the \\'Help\\' section. Select \\'Contact Us\\' and choose your preferred method: email, phone, or live chat. Our support team is available 24/7 to assist you with any issues. Additionally, you can find a FAQ section that may answer your questions without needing to contact support.\",\\n     \"incorrect\", \"adds new information (24/7 availability, FAQ section)\"],\\n  \\n    [\"What should I do if my card is lost or stolen?\",\\n     \"If your card is lost or stolen, immediately log into your account, go to \\'Card Management\\', and select \\'Report Lost/Stolen\\'. Follow the instructions to block your card and request a replacement. You can also contact our support team for assistance.\",\\n     \"If your card is lost or stolen, navigate to \\'Card Management\\' in your account, and select \\'Report Lost/Stolen\\'. Follow the prompts to block your card and request a replacement. Additionally, you can contact our support team for help.\",\\n     \"correct\", \"\"],\\n  \\n    [\"How do I enable two-factor authentication (2FA)?\",\\n     \"To enable two-factor authentication, log into your account, go to \\'Security Settings\\', and select \\'Enable 2FA\\'. Follow the instructions to link your account with a 2FA app like Google Authenticator. Once set up, you will need to enter a code from the app each time you log in.\",\\n     \"To enable two-factor authentication, log into your account, navigate to \\'Security Settings\\', and choose \\'Enable 2FA\\'. Follow the on-screen instructions to link your account with a 2FA app such as Google Authenticator. After setup, each login will require a code from the app. Additionally, you can set up backup codes in case you lose access to the 2FA app.\",\\n     \"incorrect\", \"adds new information (backup codes)\"]\\n  ]\\n  \\n  columns = [\"question\", \"target_response\", \"new_response\", \"label\", \"comment\"]\\n  \\n  golden_dataset = pd.DataFrame(data, columns=columns)\\n  ```\\n</Accordion>\\n\\n<Note>\\n  **Synthetic data**. You can also generate example inputs for your LLM app using [Evidently Platform](/docs/platform/datasets_generate).\\n</Note>\\n\\n**Create an Evidently dataset object.** Pass the dataframe and [map the column types](/docs/library/data_definition):\\n\\n```python\\ndefinition = DataDefinition(\\n    text_columns=[\"question\", \"target_response\", \"new_response\"],\\n    categorical_columns=[\"label\"]\\n    )\\n\\neval_dataset = Dataset.from_pandas(\\n    golden_dataset,\\n    data_definition=definition)\\n```\\n\\nTo preview the dataset:\\n\\n```python\\npd.set_option(\\'display.max_colwidth\\', None)\\ngolden_dataset.head(5)\\n```\\n\\n![](/images/examples/llm_judge_tutorial',\n",
       "  'title': 'LLM as a judge',\n",
       "  'description': 'How to create and evaluate an LLM judge.',\n",
       "  'filename': 'examples/LLM_judge.mdx',\n",
       "  'chunk_id': 7}]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk_index.search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b43bc4d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gitsource import chunk_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "40c9890c",
   "metadata": {},
   "outputs": [],
   "source": [
    "document_chunks = chunk_documents(documents, size=3000, step=1500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "10812857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'start': 9000,\n",
       " 'content': 'cation=[BinaryClassification(\\n        target=\"target\",\\n        prediction_labels=\"prediction\")],\\n    categorical_columns=[\"target\", \"prediction\"])\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    target: str = \"target\"\\n    prediction_labels: Optional[str] = None\\n    prediction_probas: Optional[str] = \"prediction\" #if probabilistic classification\\n    pos_label: Label = 1 #name of the positive label\\n    labels: Optional[Dict[Label, str]] = None\\n```\\n\\n### Ranking\\n\\n#### RecSys\\n\\nTo evaluate recommender systems performance, you must map the columns with:\\n\\n- Prediction: this could be predicted score or rank.\\n- Target: relevance labels (e.g., this could be an interaction result like user click or upvote, or a true relevance label)\\n\\nThe **target** column can contain either:\\n\\n- a binary label (where `1` is a positive outcome)\\n- any scores (positive values, where a higher value corresponds to a better match or a more valuable user action).\\n\\nHere are the examples of the expected data inputs.\\n\\nIf the system prediction is a **score** (expected by default):\\n\\n| user_id | item_id | prediction (score) | target (relevance) |\\n| ------- | ------- | ------------------ | ------------------ |\\n| user_1  | item_1  | 1.95               | 0                  |\\n| user_1  | item_2  | 0.8                | 1                  |\\n| user_1  | item_3  | 0.05               | 0                  |\\n\\nIf the model prediction is a **rank**:\\n\\n| user_id | item_id | prediction (rank) | target (relevance) |\\n| ------- | ------- | ----------------- | ------------------ |\\n| user_1  | item_1  | 1                 | 0                  |\\n| user_1  | item_2  | 2                 | 1                  |\\n| user_1  | item_3  | 3                 | 0                  |\\n\\nExample mapping:\\n\\n```python\\ndefinition = DataDefinition(\\n    ranking=[Recsys()]\\n    )\\n```\\n\\nAvailable options and defaults:\\n\\n```python\\n    user_id: str = \"user_id\" #columns with user IDs\\n    item_id: str = \"item_id\" #columns with ranked items\\n    target: str = \"target\"\\n    prediction: str = \"prediction\"\\n```',\n",
       " 'title': 'Data definition',\n",
       " 'description': 'How to map the input data.',\n",
       " 'filename': 'docs/library/data_definition.mdx'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_chunks[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3273b769",
   "metadata": {},
   "source": [
    "# RAG implemented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4d876d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "df449131",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'how do I implement llm as a judge?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c0a0ef59",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = chunk_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5df67e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5355dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_result_json = json.dumps(search_results, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ac88153f",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're a course assistant, your task is to answer the QUESTION from the\n",
    "course students using the provided CONTEXT\n",
    "\"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"\n",
    "<QUESTION>\n",
    "{query}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{search_result_json}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "48c18e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(user_prompt, instructions=None, model='gpt-4o-mini'):\n",
    "\n",
    "    messages = []\n",
    "\n",
    "    if instructions is not None:\n",
    "        messages.append({'role': 'system', 'content': instructions})\n",
    "\n",
    "    messages.append({'role': 'user', 'content': user_prompt})\n",
    "\n",
    "    response = openai_client.responses.create(\n",
    "        model=model,\n",
    "        input=messages\n",
    "    )\n",
    "\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "09fa8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm(user_prompt,instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d442eabc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To implement an LLM as a judge, follow these steps:\n",
      "\n",
      "1. **Install Required Libraries**:\n",
      "   You need to install the Evidently library. Use the command:\n",
      "   ```python\n",
      "   pip install evidently\n",
      "   ```\n",
      "\n",
      "2. **Set Up Your Environment**:\n",
      "   Import necessary modules and set your OpenAI API key:\n",
      "   ```python\n",
      "   import os\n",
      "   os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\n",
      "   ```\n",
      "\n",
      "3. **Create a Dataset**:\n",
      "   Generate a toy Q&A dataset that includes:\n",
      "   - **Questions**: Inputs sent to the LLM.\n",
      "   - **Target Responses**: Approved accurate responses.\n",
      "   - **New Responses**: The generated responses to be evaluated.\n",
      "   - **Manual Labels**: Labels indicating if responses are correct or not.\n",
      "\n",
      "   Here’s an example of how to create this dataset:\n",
      "   ```python\n",
      "   data = [\n",
      "       [\"question_1\", \"target_response_1\", \"new_response_1\", \"manual_label_1\"],\n",
      "       # Add more entries as needed\n",
      "   ]\n",
      "   ```\n",
      "\n",
      "4. **Create an LLM Evaluator**: \n",
      "   Define a prompt designed for evaluating responses. You can utilize pre-defined templates from the library:\n",
      "   ```python\n",
      "   from evidently.llm.templates import BinaryClassificationPromptTemplate\n",
      "\n",
      "   prompt_template = BinaryClassificationPromptTemplate(\n",
      "       # Specify your criteria and settings\n",
      "   )\n",
      "   ```\n",
      "\n",
      "5. **Add Descriptors to Your Dataset**: \n",
      "   Incorporate the LLM evaluator into your dataset:\n",
      "   ```python\n",
      "   eval_dataset.add_descriptors(\n",
      "       descriptors=[\n",
      "           LLMEval(\"new_response\", template=prompt_template, provider=\"openai\", model=\"gpt-4o-mini\")\n",
      "       ]\n",
      "   )\n",
      "   ```\n",
      "\n",
      "6. **Run Evaluations**: \n",
      "   Generate and view the results. You can create a report using the following code:\n",
      "   ```python\n",
      "   report = Report([TextEvals()])\n",
      "   my_eval = report.run(eval_dataset, None)\n",
      "   ```\n",
      "\n",
      "7. **Evaluate the Judge's Quality**: \n",
      "   Assess how effective your LLM evaluator is by comparing its outputs to your manual labels. Treat it as a binary classification problem and calculate relevant metrics.\n",
      "\n",
      "8. **Iterate and Improve**: \n",
      "   Adjust your prompts and the evaluation criteria based on the results you get. Consider trying different LLMs to see which performs better in your context.\n",
      "\n",
      "For more detailed instructions and examples, consider referring to your tutorial documents or running sample notebooks available in Jupyter or Colab.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "46b0da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    return chunk_index.search(query, num_results=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "f2bd0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"\"\"\n",
    "You're a course assistant, your task is to answer the QUESTION from the\n",
    "course students using the provided CONTEXT\n",
    "\"\"\"\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    search_result_json = json.dumps(search_results, indent=2)\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "    <QUESTION>\n",
    "    {query}\n",
    "    </QUESTION>\n",
    "\n",
    "    <CONTEXT>\n",
    "    {search_result_json}\n",
    "    </CONTEXT>\n",
    "    \"\"\".strip()\n",
    "\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "16696ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt, instructions)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "249d344c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To implement an LLM as a judge, follow these steps based on the provided context:\\n\\n1. **Prerequisites**:\\n   - Ensure you have basic Python knowledge.\\n   - Obtain an OpenAI API key to use for the LLM evaluator.\\n\\n2. **Set Up Your Environment**:\\n   - Install the Evidently library:\\n     ```python\\n     pip install evidently\\n     ```\\n   - Import the required Python modules:\\n     ```python\\n     import pandas as pd\\n     import numpy as np\\n     from evidently import Dataset, DataDefinition, Report\\n     from evidently.metrics import *\\n     from evidently.llm.templates import BinaryClassificationPromptTemplate\\n     ```\\n\\n3. **Configure Your OpenAI API Key**:\\n   - Set your OpenAI API key as an environment variable:\\n     ```python\\n     import os\\n     os.environ[\"OPENAI_API_KEY\"] = \"YOUR_KEY\"\\n     ```\\n\\n4. **Create an Evaluation Dataset**:\\n   - Construct a toy Q&A dataset that includes:\\n     - Questions (inputs sent to the LLM).\\n     - Target responses (approved answers).\\n     - New responses (imitated answers).\\n     - Manual labels (indicating correctness).\\n   \\n   Example to generate the dataset:\\n   ```python\\n   data = [\\n       [\"Question?\", \"Correct Answer.\", \"New Response.\", \"label\", \"explanation\"],\\n       # Add more entries as needed\\n   ]\\n   ```\\n\\n5. **Design the LLM Judge Prompt**:\\n   - Use templates to instruct the LLM on how to evaluate responses. For instance:\\n   ```python\\n   eval_dataset.add_descriptors(descriptors=[\\n       LLMEval(\\n           \"new_response\",\\n           template=correctness,\\n           provider=\"openai\",\\n           model=\"gpt-4o-mini\",\\n           alias=\"Correctness\",\\n           additional_columns={\"target_response\": \"target_response\"}\\n       )\\n   ])\\n   ```\\n\\n6. **Run the Evaluation**:\\n   - Generate a report to summarize the results:\\n   ```python\\n   report = Report([TextEvals()])\\n   my_eval = report.run(eval_dataset, None)\\n   ```\\n\\n7. **Evaluate the Quality of the LLM Judge**:\\n   - Use the manual labels and LLM outputs to assess the efficacy of your LLM judge, treating it as a binary classification task:\\n   ```python\\n   definition = DataDefinition(\\n       classification=[BinaryClassification(target=\"label\", prediction_labels=\"Correctness\", pos_label=\"incorrect\")]\\n   )\\n   ```\\n\\n8. **Explore Results**:\\n   - Review the evaluation results and adjust your prompts or dataset based on feedback.\\n\\nFor more detailed steps, including code examples and specific adjustments, consider running the tutorial in a Jupyter Notebook or Google Colab, as suggested in the context.'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag('how do i implement llm as a judge?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de1397e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-engineering-buildcamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
